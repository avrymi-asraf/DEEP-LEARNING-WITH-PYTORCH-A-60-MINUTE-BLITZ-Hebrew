{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## הקדמה בסיסית `torch.autograd`\n",
    "\n",
    "`torch.autograd` הוא מנוע בידול? אוטומטי שמאפשר לאמן רשתות נוירונים. בפרק הזה ננסה להסביר בצורה כללית את איך `torch.autograd` מאפשר לאמן רשתות נויירונים\n",
    "### טנזורים ורשת נוירונים _nerual networks_\n",
    "> רשת נוירונים  _nerual networks (NN)_ היא קבוצה של פונקציות משורשרות אחת בתוך השניה, שמפעילים אותה על מידע כלשהוא ע\"מ לחזות את העתיד (ע\"פ רוב).  אותם פונקציות מוגדרות על ידי פרמטרים שנקראים _biases_ או _weights_ שאותם _PyTorch_  מאחסנת בטנזורים. \n",
    "\n",
    "ע\"מ שNN יפעל כמו שצריך, כלומר יהיה לנו את ה_weights_ (משקולות) הנכונים, צריך לאמן את הרשת.\n",
    "ע\"מ לאמן את הרשת צריך שיהיה לנו מידע _input_ ותוצאות שידוע שהם נכונות ? ואז ניתן לעשות את שלב האימון:\n",
    "\n",
    "**פיעפוע לפנים? _Forward Propagation_**\n",
    "בתהליך זה האלגוריתם לוקח את המשקולות הנתונים לו (בצעד הראשון אלו משקולות אקראיים) ומחשב את התוצאה עבור מידע מסויים. \n",
    "\n",
    "**פיעפוע לאחר _Backward Propagation_**\n",
    "בתהליך זה האלגוריתם משווה את התוצאה עם התוצאה הידועה לו כנכונה, ומעדכן את המשקולות באופן פפרציונלי לשגיאה. הוא עושה את זה ע\"י מעבר אחורה על הפונקציות ושינוי של הפרמטרים שלהם ע\"מ שיתקרבו לתשובה הנכונה - ע\"י מבט על הנגזרת שלהם. פונקציה זאת נקראת _gradient descent_. ע\"מ להבין טוב יותר ניתן לראות את הסרטון [הזה](https://www.youtube.com/watch?v=tIeHLnjs5U8) של _3Blue1Brown_  \n",
    "\n",
    "\n",
    "\n",
    "## אימון עם _PyTorch_\n",
    "\n",
    "\n",
    "נתבונן על מחזור אימון אחד. \n",
    "בדוגמה הזאת נטען מודל מאומן מראש בשם `resnet18` מתוך חבילת המודלים המאומנים `torchvision`. אנחנו ניצור טנזור רנדומלי `data` שייצג תמונה עם 3 ערוצים (RBG), הגובה והרוחב של התמונה הם 64X64 פיקסלים.\n",
    "לתמונה האקראית ניצור גם `label` אקראי שייצג את מה שיש בתמונה. הצורה של המודלים היא (1,1000) ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64) #ramdom image\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "נוכל לראות את כל שכבה ואת המאפיינים שלה ע\"י מעבר על `()model.named_parameters` (יש המון מידע בשכבות, אז לא כדאי להדפיס את כל המשקלים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight-> True\n",
      "bn1.weight-> True\n",
      "bn1.bias-> True\n",
      "layer1.0.conv1.weight-> True\n",
      "layer1.0.bn1.weight-> True\n",
      "layer1.0.bn1.bias-> True\n",
      "layer1.0.conv2.weight-> True\n",
      "layer1.0.bn2.weight-> True\n",
      "layer1.0.bn2.bias-> True\n",
      "layer1.1.conv1.weight-> True\n",
      "layer1.1.bn1.weight-> True\n",
      "layer1.1.bn1.bias-> True\n",
      "layer1.1.conv2.weight-> True\n",
      "layer1.1.bn2.weight-> True\n",
      "layer1.1.bn2.bias-> True\n",
      "layer2.0.conv1.weight-> True\n",
      "layer2.0.bn1.weight-> True\n",
      "layer2.0.bn1.bias-> True\n",
      "layer2.0.conv2.weight-> True\n",
      "layer2.0.bn2.weight-> True\n",
      "layer2.0.bn2.bias-> True\n",
      "layer2.0.downsample.0.weight-> True\n",
      "layer2.0.downsample.1.weight-> True\n",
      "layer2.0.downsample.1.bias-> True\n",
      "layer2.1.conv1.weight-> True\n",
      "layer2.1.bn1.weight-> True\n",
      "layer2.1.bn1.bias-> True\n",
      "layer2.1.conv2.weight-> True\n",
      "layer2.1.bn2.weight-> True\n",
      "layer2.1.bn2.bias-> True\n",
      "layer3.0.conv1.weight-> True\n",
      "layer3.0.bn1.weight-> True\n",
      "layer3.0.bn1.bias-> True\n",
      "layer3.0.conv2.weight-> True\n",
      "layer3.0.bn2.weight-> True\n",
      "layer3.0.bn2.bias-> True\n",
      "layer3.0.downsample.0.weight-> True\n",
      "layer3.0.downsample.1.weight-> True\n",
      "layer3.0.downsample.1.bias-> True\n",
      "layer3.1.conv1.weight-> True\n",
      "layer3.1.bn1.weight-> True\n",
      "layer3.1.bn1.bias-> True\n",
      "layer3.1.conv2.weight-> True\n",
      "layer3.1.bn2.weight-> True\n",
      "layer3.1.bn2.bias-> True\n",
      "layer4.0.conv1.weight-> True\n",
      "layer4.0.bn1.weight-> True\n",
      "layer4.0.bn1.bias-> True\n",
      "layer4.0.conv2.weight-> True\n",
      "layer4.0.bn2.weight-> True\n",
      "layer4.0.bn2.bias-> True\n",
      "layer4.0.downsample.0.weight-> True\n",
      "layer4.0.downsample.1.weight-> True\n",
      "layer4.0.downsample.1.bias-> True\n",
      "layer4.1.conv1.weight-> True\n",
      "layer4.1.bn1.weight-> True\n",
      "layer4.1.bn1.bias-> True\n",
      "layer4.1.conv2.weight-> True\n",
      "layer4.1.bn2.weight-> True\n",
      "layer4.1.bn2.bias-> True\n",
      "fc.weight-> True\n",
      "fc.bias-> True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, end = \"-> \")\n",
    "    print(param.requires_grad) #if need to update it in Backward Propagation\n",
    "#     print(param) #All the weights of the nets it's huge\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בשלב הבא נעביר את הקלט (התמונה האקראית שיצרנו) דרך כל אחת משכבות המודל, נקבל בפלט חיזוי מהו תוכן התמונה.\n",
    "פלט החיזוי הוא טנזור שמכיל לכל ליבל את הסיכוי שזה הוא. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "נשווה את מה שהמודל חזה ואת מה שידוע לנו שהיא התוצאה הנכונה, נסכום את ההפרש בינהם כך נוכל לחשב את שיעור הטעות `loss`, בשלב הבא נשנה את המשקולות בהתאמה בתהליך הפיעפוע לאחור _backward_ \n",
    "\n",
    "תהליך הפיעפוע לאחור מתחיל ע\"י קריאה למתודה `()backward.` על הטנזור הטעות `loss`. לאחר מכן בצורה אוטומטית _Autograd_ מחשב ומאחסן את הגרדיאנט _gradients_ לכל מודל פרמטר של המודל בתוך המאפיין (_attribute_) `grad.`  של הפרמטר.  \n",
    ">💡 שימו לב - המתודה `()backward.` נגשת לכל שכבה במודל ומכניסה לה את ערך ה_grad_, למרות שהיא נקראת על טנזור ה_loss_  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בשלב הבא נפעיל את תהליך האופטימיזציה על המודל, במקרה הזה נשתמש ב_SGD_ כך שהפרמטרים _learninig rate_ `lr` הוא 0.01 והמומנטום [_momentum_]((https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) הוא 0.9, נכניס את כל הפרמטרים לתוך האופטימייזר.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "לבסוף קוראים למתודה `()step.` ע\"מ להחיל את ה_gradient descent_ על הפרמטרים. האפטימייזר ישנה את הערכים של כל פרמטר ופרמטר לפי הגרינאנט _gradient_ שנמצא ב`grad.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() #gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "בשלב הזה סיימנו את כל התהליך של אימון רשת נוירונים. בהמשך הפרק נראה איך הדברים פועלים מבחינה מתמטית. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "## מה זה גרדיאנט _gradient_\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### חישוב אוטומטי של ה`grad.` ע\"י _autograd_\n",
    "נראה כיצד _autograd_  כותב את ערכי ה`grad.`\n",
    "ניצור שני טנזורים _a_ ו_b_ עם השדה`requires_grad=True` כך נסמן ל_autograd_ שצריך לעקוב אחרי כל השינויים שקוראים בטנזור הזה "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ניצור טנזור אחר _Q_ מ_a_ ו_b_\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "נניח ש_a_ ו_b_ הם פרמטרים של רשת נויירונים כל שהיא ו_Q_ הוא טנזור השגיאה _loss_, באימון הרשת אנחנו רוצים לחשב את גרדיאנט של הפרמטרים, כלומר הנגזרת של הפער בן הטנזור הנוכחי לטנזור השגיאה לפי המשתנים השונים\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "כאשר אנחנו קוראים למתודה `()backward.` על _Q_ בצורה אוטמטית _autograd_ מחשבת את הגרדיאנט ומאחסנת אותו בתוך המאפיין (_attribute_)  `grad.` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "אנחנו צריכים בצורה מפורשת להעביר טנזור _gradient_ כארגומנט לתוך `()Q.backward`  בגלל שהוא וקטור, הוא מייצג את הגרדיאנט של _Q_, מכיון ביחס לעצמו - ששווה לטנזור אחדות\n",
    "\n",
    "\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n",
    "\n",
    "לחלופין ניתן לסכום את אברי _Q_ ואז לקרוא ל`()Q.sum().backward` בלי להעביר את הגרדיאנט של השגיאה"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients are now deposited in ``a.grad`` and ``b.grad``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Reading - Vector Calculus using ``autograd``\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Mathematically, if you have a vector valued function\n",
    "$\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with\n",
    "respect to $\\vec{x}$ is a Jacobian matrix $J$:\n",
    "\n",
    "\\begin{align}J\n",
    "     =\n",
    "      \\left(\\begin{array}{cc}\n",
    "      \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n",
    "      ... &\n",
    "      \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\n",
    "     =\n",
    "     \\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "Generally speaking, ``torch.autograd`` is an engine for computing\n",
    "vector-Jacobian product. That is, given any vector $\\vec{v}$, compute the product\n",
    "$J^{T}\\cdot \\vec{v}$\n",
    "\n",
    "If $\\vec{v}$ happens to be the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$:\n",
    "\n",
    "\\begin{align}\\vec{v}\n",
    "   =\n",
    "   \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\end{align}\n",
    "\n",
    "then by the chain rule, the vector-Jacobian product would be the\n",
    "gradient of $l$ with respect to $\\vec{x}$:\n",
    "\n",
    "\\begin{align}J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "      \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "      \\vdots\\\\\n",
    "      \\frac{\\partial l}{\\partial y_{m}}\n",
    "      \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "      \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "      \\vdots\\\\\n",
    "      \\frac{\\partial l}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "This characteristic of vector-Jacobian product is what we use in the above example;\n",
    "``external_grad`` represents $\\vec{v}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational Graph\n",
    "~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) & all executed\n",
    "operations (along with the resulting new tensors) in a directed acyclic\n",
    "graph (DAG) consisting of\n",
    "`Function <https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function>`__\n",
    "objects. In this DAG, leaves are the input tensors, roots are the output\n",
    "tensors. By tracing this graph from roots to leaves, you can\n",
    "automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "\n",
    "- run the requested operation to compute a resulting tensor, and\n",
    "- maintain the operation’s *gradient function* in the DAG.\n",
    "\n",
    "The backward pass kicks off when ``.backward()`` is called on the DAG\n",
    "root. ``autograd`` then:\n",
    "\n",
    "- computes the gradients from each ``.grad_fn``,\n",
    "- accumulates them in the respective tensor’s ``.grad`` attribute, and\n",
    "- using the chain rule, propagates all the way to the leaf tensors.\n",
    "\n",
    "Below is a visual representation of the DAG in our example. In the graph,\n",
    "the arrows are in the direction of the forward pass. The nodes represent the backward functions\n",
    "of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors ``a`` and ``b``.\n",
    "\n",
    ".. figure:: /_static/img/dag_autograd.png\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>**DAGs are dynamic in PyTorch**\n",
    "  An important thing to note is that the graph is recreated from scratch; after each\n",
    "  ``.backward()`` call, autograd starts populating a new graph. This is\n",
    "  exactly what allows you to use control flow statements in your model;\n",
    "  you can change the shape, size and operations at every iteration if\n",
    "  needed.</p></div>\n",
    "\n",
    "Exclusion from the DAG\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "``torch.autograd`` tracks operations on all tensors which have their\n",
    "``requires_grad`` flag set to ``True``. For tensors that don’t require\n",
    "gradients, setting this attribute to ``False`` excludes it from the\n",
    "gradient computation DAG.\n",
    "\n",
    "The output tensor of an operation will require gradients even if only a\n",
    "single input tensor has ``requires_grad=True``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a NN, parameters that don't compute gradients are usually called **frozen parameters**.\n",
    "It is useful to \"freeze\" part of your model if you know in advance that you won't need the gradients of those parameters\n",
    "(this offers some performance benefits by reducing autograd computations).\n",
    "\n",
    "Another common usecase where exclusion from the DAG is important is for\n",
    "`finetuning a pretrained network <https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html>`__\n",
    "\n",
    "In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels.\n",
    "Let's walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to finetune the model on a new dataset with 10 labels.\n",
    "In resnet, the classifier is the last linear layer ``model.fc``.\n",
    "We can simply replace it with a new linear layer (unfrozen by default)\n",
    "that acts as our classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all parameters in the model, except the parameters of ``model.fc``, are frozen.\n",
    "The only parameters that compute gradients are the weights and bias of ``model.fc``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice although we register all the parameters in the optimizer,\n",
    "the only parameters that are computing gradients (and hence updated in gradient descent)\n",
    "are the weights and bias of the classifier.\n",
    "\n",
    "The same exclusionary functionality is available as a context manager in\n",
    "`torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html>`__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further readings:\n",
    "~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "-  `In-place operations & Multithreaded Autograd <https://pytorch.org/docs/stable/notes/autograd.html>`__\n",
    "-  `Example implementation of reverse-mode autodiff <https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC>`__\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "direction": "rtl",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
